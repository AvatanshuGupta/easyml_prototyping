{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c974c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33bdea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Classifier:\n",
    "    def __init__(self,epochs=1000,learning_rate=0.1,patience=20,min_del=1e-4):\n",
    "        self.w=None\n",
    "        self.b=None\n",
    "        self.no_of_class=None\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.patience=patience\n",
    "        self.min_del=min_del\n",
    "\n",
    "    def softmax(self,z):\n",
    "        num=np.exp(z-np.max(z,axis=1,keepdims=True))\n",
    "        return num/np.sum(num,axis=1,keepdims=True)\n",
    "    \n",
    "    def one_hot(self,y):\n",
    "        one_hot=np.zeros((y.shape[0],self.no_of_class))\n",
    "        for i in range(y.shape[0]):\n",
    "            one_hot[i,y[i]]=1\n",
    "        return one_hot\n",
    "    \n",
    "    def compute_loss(self,y_true,y_pred):\n",
    "        m=y_true.shape[0]\n",
    "        return -np.sum(y_true*np.log(y_pred+1e-4))/m\n",
    "    \n",
    "\n",
    "    def fit(self,X,y):\n",
    "        # m-row n-column\n",
    "        # y is m-row 1-column\n",
    "        m,n=X.shape\n",
    "        # number of classes\n",
    "        self.no_of_class=np.max(y)+1\n",
    "        # weight n*k(no. of class)\n",
    "        self.w=np.zeros((n,self.no_of_class))\n",
    "        # b 1*k\n",
    "        self.b=np.zeros((1,self.no_of_class))\n",
    "        epoch_no=0\n",
    "        best_loss=float('inf')\n",
    "        for epoch in range(self.epochs):\n",
    "            # z m*k\n",
    "            z=np.dot(X,self.w)+self.b\n",
    "            # y_hat m*k\n",
    "            y_hat=self.softmax(z)\n",
    "            # m*c\n",
    "            y_true=self.one_hot(y)\n",
    "\n",
    "            loss=self.compute_loss(y_true,y_hat)\n",
    "            if best_loss-loss>=self.min_del:\n",
    "                best_loss=loss\n",
    "                epoch_no=0\n",
    "                \n",
    "            else:\n",
    "                epoch_no+=1\n",
    "                \n",
    "            if epoch_no>=self.patience:\n",
    "                print(f\"No improvement since {self.patience} epochs.Performing early stoping at {epoch}...\")\n",
    "                break\n",
    "\n",
    "\n",
    "            # applying gradient descent\n",
    "            self.w-=self.learning_rate*np.dot(X.T,(y_hat-y_true))/m\n",
    "            self.b-=np.sum(self.learning_rate*(y_hat-y_true),keepdims=True)/m\n",
    "        \n",
    "    def predict(self,X):\n",
    "        z=np.dot(X,self.w)+self.b\n",
    "        prob=self.softmax(z)\n",
    "        return np.argmax(prob,axis=1)\n",
    "    \n",
    "    def predict_prob(self,X):\n",
    "        z=np.dot(X,self.w)+self.b\n",
    "        return self.softmax(z)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bbaf43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement since 30 epochs.Performing early stoping at 342...\n",
      "Custom Softmax Classifier:\n",
      " - Accuracy: 0.5785\n",
      " - Training Time: 2.54 seconds\n",
      "\n",
      "Sklearn LogisticRegression:\n",
      " - Accuracy: 0.5770\n",
      " - Training Time: 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avata\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=10000,     \n",
    "                           n_features=100,         \n",
    "                           n_informative=30,     \n",
    "                           n_redundant=10,\n",
    "                           n_classes=5,          \n",
    "                           random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "# custom model\n",
    "custom_model = Softmax_Classifier(epochs=1000, learning_rate=0.1,patience=30,min_del=0.001)\n",
    "start = time()\n",
    "custom_model.fit(X_train, y_train)\n",
    "custom_time = time() - start\n",
    "custom_acc = np.mean(custom_model.predict(X_test) == y_test)\n",
    "\n",
    "# sklearn model\n",
    "sklearn_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "start = time()\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_time = time() - start\n",
    "sklearn_acc = sklearn_model.score(X_test, y_test)\n",
    "\n",
    "#Print\n",
    "print(\"Custom Softmax Classifier:\")\n",
    "print(f\" - Accuracy: {custom_acc:.4f}\")\n",
    "print(f\" - Training Time: {custom_time:.2f} seconds\\n\")\n",
    "\n",
    "print(\"Sklearn LogisticRegression:\")\n",
    "print(f\" - Accuracy: {sklearn_acc:.4f}\")\n",
    "print(f\" - Training Time: {sklearn_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79699c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6786911 , 0.03382355, 0.10000227, 0.16572225, 0.02176084],\n",
       "       [0.21382901, 0.04190716, 0.19071795, 0.2427534 , 0.31079248],\n",
       "       [0.03032151, 0.63985169, 0.01325852, 0.07195761, 0.24461067],\n",
       "       ...,\n",
       "       [0.10836465, 0.33606695, 0.43920993, 0.05525866, 0.0610998 ],\n",
       "       [0.0759175 , 0.0530696 , 0.25260778, 0.03352222, 0.58488289],\n",
       "       [0.01706147, 0.27449905, 0.42383419, 0.01004779, 0.2745575 ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.predict_prob(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5544521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "344a717b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.92387642e-17, 2.92387642e-17, 2.92387642e-17, 2.92387642e-17,\n",
       "        2.92387642e-17]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
