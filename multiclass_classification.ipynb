{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c974c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33bdea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Classifier:\n",
    "    def __init__(self,epochs=1000,learning_rate=0.1):\n",
    "        self.w=None\n",
    "        self.b=None\n",
    "        self.no_of_class=None\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "    def softmax(self,z):\n",
    "        num=np.exp(z-np.max(z,axis=1,keepdims=True))\n",
    "        return num/np.sum(num,axis=1,keepdims=True)\n",
    "    \n",
    "    def one_hot(self,y):\n",
    "        one_hot=np.zeros((y.shape[0],self.no_of_class))\n",
    "        for i in range(y.shape[0]):\n",
    "            one_hot[i,y[i]]=1\n",
    "        return one_hot\n",
    "    \n",
    "\n",
    "    def fit(self,X,y):\n",
    "        # m-row n-column\n",
    "        # y is m-row 1-column\n",
    "        m,n=X.shape\n",
    "        # number of classes\n",
    "        self.no_of_class=np.max(y)+1\n",
    "        # weight n*k(no. of class)\n",
    "        self.w=np.zeros((n,self.no_of_class))\n",
    "        # b 1*k\n",
    "        self.b=np.zeros((1,self.no_of_class))\n",
    "        for epoch in range(self.epochs):\n",
    "            # z m*k\n",
    "            z=np.dot(X,self.w)+self.b\n",
    "            # y_hat m*k\n",
    "            y_hat=self.softmax(z)\n",
    "            # m*c\n",
    "            y_true=self.one_hot(y)\n",
    "            # applying gradient descent\n",
    "            self.w-=self.learning_rate*np.dot(X.T,(y_hat-y_true))/m\n",
    "            self.b-=np.sum(self.learning_rate*(y_hat-y_true),keepdims=True)/m\n",
    "        \n",
    "    def predict(self,X):\n",
    "        z=np.dot(X,self.w)+self.b\n",
    "        prob=self.softmax(z)\n",
    "        return np.argmax(prob,axis=1)\n",
    "    \n",
    "    def predict_prob(self,X):\n",
    "        z=np.dot(X,self.w)+self.b\n",
    "        return self.softmax(z)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bbaf43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Softmax Classifier:\n",
      " - Accuracy: 0.5900\n",
      " - Training Time: 4.08 seconds\n",
      "\n",
      "Sklearn LogisticRegression:\n",
      " - Accuracy: 0.5865\n",
      " - Training Time: 0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avata\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from time import time\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=10000,     \n",
    "                           n_features=50,         \n",
    "                           n_informative=30,     \n",
    "                           n_redundant=10,\n",
    "                           n_classes=5,          \n",
    "                           random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "# custom model\n",
    "custom_model = Softmax_Classifier(epochs=1000, learning_rate=0.1)\n",
    "start = time()\n",
    "custom_model.fit(X_train, y_train)\n",
    "custom_time = time() - start\n",
    "custom_acc = np.mean(custom_model.predict(X_test) == y_test)\n",
    "\n",
    "# sklearn model\n",
    "sklearn_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "start = time()\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_time = time() - start\n",
    "sklearn_acc = sklearn_model.score(X_test, y_test)\n",
    "\n",
    "#Print\n",
    "print(\"Custom Softmax Classifier:\")\n",
    "print(f\" - Accuracy: {custom_acc:.4f}\")\n",
    "print(f\" - Training Time: {custom_time:.2f} seconds\\n\")\n",
    "\n",
    "print(\"Sklearn LogisticRegression:\")\n",
    "print(f\" - Accuracy: {sklearn_acc:.4f}\")\n",
    "print(f\" - Training Time: {sklearn_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
